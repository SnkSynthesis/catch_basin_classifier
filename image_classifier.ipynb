{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catch Basin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some initial setup code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras import layers\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Average Image Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = []\n",
    "heights = []\n",
    "\n",
    "for path in glob(\"data_sorted/**/*.JPG\"):\n",
    "    with Image.open(path) as img:\n",
    "        widths.append(img.width)\n",
    "        heights.append(img.height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(554, 732)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size = round(sum(widths) / len(widths)), round(sum(heights) / len(heights))\n",
    "image_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and split into two groups: *training* and *validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44 files belonging to 3 classes.\n",
      "Using 36 files for training.\n",
      "Found 44 files belonging to 3 classes.\n",
      "Using 8 files for validation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['blocked', 'clear', 'partial']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = tf.keras.utils.image_dataset_from_directory(\"data_sorted\", validation_split=0.2, subset=\"training\", seed=321, image_size=image_size)\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\"data_sorted\", validation_split=0.2, subset=\"validation\", seed=321, image_size=image_size)\n",
    "\n",
    "# Get list of classnames to verify that the class names were interpreted correctly\n",
    "training_dataset.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_2 (Rescaling)     (None, 554, 732, 3)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 554, 732, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 277, 366, 16)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 277, 366, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 138, 183, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 138, 183, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 69, 91, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 401856)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               51437696  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 51,461,667\n",
      "Trainable params: 51,461,667\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(*image_size, 3)),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='tanh'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='tanh'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='tanh'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(3)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2/2 [==============================] - 8s 2s/step - loss: 1.2463 - accuracy: 0.6944 - val_loss: 6.0840 - val_accuracy: 0.2500\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 9s 2s/step - loss: 1.0678 - accuracy: 0.6944 - val_loss: 4.6682 - val_accuracy: 0.1250\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 14s 3s/step - loss: 0.4934 - accuracy: 0.8056 - val_loss: 3.0775 - val_accuracy: 0.3750\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 14s 3s/step - loss: 0.2264 - accuracy: 0.9444 - val_loss: 3.7537 - val_accuracy: 0.5000\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 16s 3s/step - loss: 0.6520 - accuracy: 0.7500 - val_loss: 3.2824 - val_accuracy: 0.3750\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 15s 3s/step - loss: 0.5856 - accuracy: 0.7500 - val_loss: 2.6962 - val_accuracy: 0.2500\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 15s 3s/step - loss: 0.4092 - accuracy: 0.7222 - val_loss: 2.8635 - val_accuracy: 0.2500\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 14s 3s/step - loss: 0.1774 - accuracy: 0.9722 - val_loss: 3.6042 - val_accuracy: 0.2500\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 11s 2s/step - loss: 0.1479 - accuracy: 0.9722 - val_loss: 4.3370 - val_accuracy: 0.2500\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 11s 2s/step - loss: 0.2364 - accuracy: 0.9167 - val_loss: 4.6768 - val_accuracy: 0.2500\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 11s 2s/step - loss: 0.3197 - accuracy: 0.8611 - val_loss: 4.5204 - val_accuracy: 0.2500\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 9s 2s/step - loss: 0.2658 - accuracy: 0.8611 - val_loss: 4.1074 - val_accuracy: 0.2500\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 10s 2s/step - loss: 0.1485 - accuracy: 0.9444 - val_loss: 3.6366 - val_accuracy: 0.2500\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 11s 2s/step - loss: 0.1015 - accuracy: 0.9722 - val_loss: 3.3245 - val_accuracy: 0.2500\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 9s 2s/step - loss: 0.1176 - accuracy: 0.9722 - val_loss: 3.1539 - val_accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "history = model.fit(\n",
    "  training_dataset,\n",
    "  validation_data=validation_dataset,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75 0.5\n",
      "0.7767490744590759 3.142123222351074\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "print(acc[-1], val_acc[-1])\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "print(loss[-1], val_loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DatasetV2.batch of <BatchDataset shapes: ((None, 554, 732, 3), (None,)), types: (tf.float32, tf.int32)>>\n",
      "[[-1.9136134   9.529867    1.9984784 ]\n",
      " [ 2.449604   10.180234    7.0497146 ]\n",
      " [ 0.87548196  9.780648    5.6230073 ]\n",
      " [-2.0325027  12.976323   10.247464  ]\n",
      " [ 4.391442    6.7047553   4.2867618 ]\n",
      " [-3.154632   14.830306    3.8983388 ]\n",
      " [ 3.1419604   3.2544844   0.01551011]\n",
      " [ 0.13148536 10.271812    0.26384118]]\n",
      "[1 2 1 1 1 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "print(validation_dataset.batch)\n",
    "for images, labels in validation_dataset.take(1):\n",
    "    npimages = images.numpy()\n",
    "    nplabels = labels.numpy()\n",
    "    results = model.predict(npimages)\n",
    "    print(results)\n",
    "    print(nplabels)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2df7cac29cf8c954e3b838d1ae23efc6dc513b8c37df1857e4dd2dc61cf098bb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

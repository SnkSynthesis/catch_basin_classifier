{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98cd7bd2-e6f7-475d-af7e-f96b45681d0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Catch Basin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e52058-e5ed-48a0-b4f3-e2b01d08a9d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "166c4133-d603-48a4-bb85-17fb6aadaf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a43a05-c8e1-4acc-a9a2-b246613f5b33",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033da29b-5d1f-433e-8d2c-350d01f876a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8\"\n",
    "DOWNLOAD_URL = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f22bfd40-b505-4aba-a96c-9f9f276b3514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model...\n",
      "Model downloaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading model...\")\n",
    "r = requests.get(DOWNLOAD_URL)\n",
    "with open(MODEL_NAME + \".tar.gz\", \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "print(\"Model downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a007e79-13da-4cc1-bb2b-fbb5fa8d5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf {MODEL_NAME + \".tar.gz\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64478de0-cd8e-4386-8ee1-05288f7d2d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(str(Path(MODEL_NAME, \"saved_model\").absolute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24c3c043-604a-4c56-a4d0-c72ac3430ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject at 0x7fe531050e20>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6382f16-592b-42ec-832f-f631c346ae8d",
   "metadata": {},
   "source": [
    "## Label Map\n",
    "`label_map.pbtxt` maps the class (name of number) to a number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99a17577-b97d-4cef-9fa8-d5872a2771a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: label_map.pbtxt: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat label_map.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e46f1c0-0a0d-4943-aae7-223b08cf70f6",
   "metadata": {},
   "source": [
    "Define some utility functions to convert from class to `int` and `int` to class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70b0866e-cabd-4cc4-935f-5012839604db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_to_int(class_name):\n",
    "    if class_name == \"blocked\":\n",
    "        return 1\n",
    "    elif class_name == \"partial\":\n",
    "        return 2\n",
    "    elif class_name == \"clear\":\n",
    "        return 3\n",
    "    else:\n",
    "        raise Exception(\"Invalid input\")\n",
    "\n",
    "\n",
    "def int_to_class(integer):\n",
    "    if integer == 1:\n",
    "        return \"blocked\"\n",
    "    elif integer == 2:\n",
    "        return \"partial\"\n",
    "    elif integer == 3:\n",
    "        return \"clear\"\n",
    "    else:\n",
    "        raise Exception(\"Invalid input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14247710-9d3e-4d12-929f-764b1e3b6cb1",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a386aef2-34f1-435e-bd68-3a44792add2d",
   "metadata": {},
   "source": [
    "Convert all PASCAL VOC (XML) files in `data/` to a CSV file, `labels.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0009df5-be8b-4358-ba49-cbc31681d18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to CSV...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import xml.etree.ElementTree as XMLElementTree\n",
    "\n",
    "print(\"Converting to CSV...\")\n",
    "columns = (\"filename\", \"class\", \"width\", \"height\", \"xmin\", \"ymin\", \"xmax\", \"ymax\")\n",
    "rows = []\n",
    "for filename in glob.glob('data/*.xml'):\n",
    "    parsed_obj = XMLElementTree.parse(filename)\n",
    "    root = parsed_obj.getroot()\n",
    "    filename = root.find(\"filename\").text\n",
    "    for obj in root.findall(\"object\"):\n",
    "        row = []\n",
    "        row.append(filename)\n",
    "        row.append(obj.find(\"name\").text)  # name => class\n",
    "        row.append(root.find(\"size\").find(\"width\").text)\n",
    "        row.append(root.find(\"size\").find(\"height\").text)\n",
    "        for i in obj.find(\"bndbox\"):\n",
    "            row.append(int(i.text))\n",
    "        rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "df.to_csv(\"labels.csv\", index=False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fdddaf-eeb1-4e71-b961-dd82ff2cea75",
   "metadata": {},
   "source": [
    "Load the CSV file with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae0a9336-05ef-475f-a4a8-7c0b19de447e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>class</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B1.JPG</td>\n",
       "      <td>blocked</td>\n",
       "      <td>659</td>\n",
       "      <td>800</td>\n",
       "      <td>316</td>\n",
       "      <td>550</td>\n",
       "      <td>458</td>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C11.JPG</td>\n",
       "      <td>clear</td>\n",
       "      <td>526</td>\n",
       "      <td>702</td>\n",
       "      <td>216</td>\n",
       "      <td>509</td>\n",
       "      <td>359</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B4.JPG</td>\n",
       "      <td>blocked</td>\n",
       "      <td>505</td>\n",
       "      <td>695</td>\n",
       "      <td>159</td>\n",
       "      <td>398</td>\n",
       "      <td>343</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C6.JPG</td>\n",
       "      <td>clear</td>\n",
       "      <td>534</td>\n",
       "      <td>695</td>\n",
       "      <td>177</td>\n",
       "      <td>458</td>\n",
       "      <td>332</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C12.JPG</td>\n",
       "      <td>clear</td>\n",
       "      <td>524</td>\n",
       "      <td>776</td>\n",
       "      <td>240</td>\n",
       "      <td>333</td>\n",
       "      <td>326</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename    class  width  height  xmin  ymin  xmax  ymax\n",
       "0   B1.JPG  blocked    659     800   316   550   458   609\n",
       "1  C11.JPG    clear    526     702   216   509   359   591\n",
       "2   B4.JPG  blocked    505     695   159   398   343   427\n",
       "3   C6.JPG    clear    534     695   177   458   332   511\n",
       "4  C12.JPG    clear    524     776   240   333   326   358"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"labels.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545ec7b-7e49-49fd-aa9e-ae093e46179a",
   "metadata": {},
   "source": [
    "Split data into test data and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2840cd52-62d5-49a3-8bdb-b75386192320",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.sample(frac=0.8, random_state=100)\n",
    "test_df = df.drop(train_df.index).sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dd3281-a7ab-4d20-9c98-96559ef26729",
   "metadata": {},
   "source": [
    "Create TF Records within `annotations/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f49d4e20-381e-4686-a0f4-4e4e83b17730",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"annotations\").exists():\n",
    "    !mkdir annotations\n",
    "\n",
    "def create_tf_record(dataframe, record_filename):\n",
    "    with tf.io.TFRecordWriter(str(Path(\"annotations\", record_filename))) as writer:\n",
    "        for index, row in dataframe.iterrows():\n",
    "            filename_encoded = row[\"filename\"].encode(\"utf-8\")\n",
    "            width = int(row[\"width\"])\n",
    "            height = int(row[\"height\"])\n",
    "            encoded_jpg = None\n",
    "            with tf.io.gfile.GFile(str(Path(\"data\") / row[\"filename\"]), \"rb\") as f:\n",
    "                encoded_jpg = f.read()\n",
    "            if encoded_jpg is None:\n",
    "                raise Exception(\"Unable to read image: \" + row[\"filename\"])\n",
    "\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                \"image/height\": dataset_util.int64_feature(height),\n",
    "                \"image/width\": dataset_util.int64_feature(width),\n",
    "                \"image/filename\": dataset_util.bytes_feature(filename_encoded),\n",
    "                \"image/source_id\": dataset_util.bytes_feature(filename_encoded),\n",
    "                \"image/encoded\": dataset_util.bytes_feature(encoded_jpg),\n",
    "                \"image/format\": dataset_util.bytes_feature(b\"jpg\"),\n",
    "                \"image/object/bbox/xmin\": dataset_util.float_list_feature([int(row[\"xmin\"]) / width]),\n",
    "                \"image/object/bbox/xmax\": dataset_util.float_list_feature([int(row[\"xmax\"]) / width]),\n",
    "                \"image/object/bbox/ymin\": dataset_util.float_list_feature([int(row[\"ymin\"]) / height]),\n",
    "                \"image/object/bbox/ymax\": dataset_util.float_list_feature([int(row[\"ymax\"]) / height]),\n",
    "                \"image/object/class/text\": dataset_util.bytes_list_feature([row[\"class\"].encode(\"utf-8\")]),\n",
    "                \"image/object/class/label\": dataset_util.int64_list_feature([class_to_int(row[\"class\"])]),\n",
    "            }))\n",
    "            writer.write(example.SerializeToString())\n",
    "\n",
    "# Train Data\n",
    "create_tf_record(train_df, \"train.record\")\n",
    "# Test Data\n",
    "create_tf_record(test_df, \"test.record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f2f84-d05c-48a8-a14e-c58aa12839c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Edit Model Configuration\n",
    "Edit `pipeline.config` to configure the model to be better for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b825627e-40bb-451a-bc0f-3d703e1134e8",
   "metadata": {},
   "source": [
    "Load `pipeline.config` into a python object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc1e2be-1535-4cf5-ac96-8a9a1fe72681",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(str(Path(MODEL_NAME, \"pipeline.config\")), \"r\") as f:\n",
    "    text_format.Merge(f.read(), pipeline_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4a0211-c2cd-463c-87ee-7f38e1648369",
   "metadata": {},
   "source": [
    "Edit attributes of `pipeline_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57645e-9eab-44f1-8097-d6a9eca87b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Classes: blocked, partial, and clear.\n",
    "pipeline_config.model.ssd.num_classes = 3\n",
    "# Set batch_size based on memory available.\n",
    "pipeline_config.train_config.batch_size = 4\n",
    "# Path to checkpoint of model\n",
    "pipeline_config.train_config.fine_tune_checkpoint = str(Path(MODEL_NAME, \"checkpoint0\", \"ckpt-0\"))\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline_config.train_input_reader.label_map_path = str(Path(\"annotations\", \"label_map.pbtxt\"))\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [str(Path(\"annotations\", \"train.record\"))]\n",
    "pipeline_config.eval_input_reader[0].label_map_path = str(Path(\"annotations\", \"label_map.pbtxt\"))\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [str(Path(\"annotations\", \"test.record\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc584754-4bbc-48ef-9f63-ae9b6df8573e",
   "metadata": {},
   "source": [
    "Save `pipeline_config` to `pipeline.config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33456dcf-d345-4156-8084-50ad6f02a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.gfile.GFile(str(Path(MODEL_NAME, \"pipeline.config\")), \"w\") as f:\n",
    "    config_text = text_format.MessageToString(pipeline_config)\n",
    "    f.write(config_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7967cf62-2336-4d4c-bab2-254d41567629",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34668a1b-f590-4acb-902c-695d622e74aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-13 12:13:38.188949: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/snksynthesis/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2021-11-13 12:13:38.189011: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-11-13 12:13:40.310438: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/snksynthesis/.local/lib/python3.8/site-packages/cv2/../../lib64:/home/snksynthesis/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2021-11-13 12:13:40.310510: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-13 12:13:40.310528: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Leni7_laptop): /proc/driver/nvidia/version does not exist\n",
      "2021-11-13 12:13:40.311010: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "W1113 12:13:40.312566 140260436539200 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "I1113 12:13:40.316620 140260436539200 mirrored_strategy.py:369] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "INFO:tensorflow:Maybe overwriting train_steps: None\n",
      "I1113 12:13:40.320076 140260436539200 config_util.py:552] Maybe overwriting train_steps: None\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I1113 12:13:40.320261 140260436539200 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/model_lib_v2.py:557: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "W1113 12:13:40.341580 140260436539200 deprecation.py:339] From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/model_lib_v2.py:557: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "INFO:tensorflow:Reading unweighted datasets: ['annotations/train.record']\n",
      "I1113 12:13:40.344696 140260436539200 dataset_builder.py:163] Reading unweighted datasets: ['annotations/train.record']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['annotations/train.record']\n",
      "I1113 12:13:40.344943 140260436539200 dataset_builder.py:80] Reading record datasets for input file: ['annotations/train.record']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I1113 12:13:40.345073 140260436539200 dataset_builder.py:81] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W1113 12:13:40.345189 140260436539200 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "W1113 12:13:40.347185 140260436539200 deprecation.py:339] From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W1113 12:13:40.459361 140260436539200 deprecation.py:339] From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W1113 12:13:46.214057 140260436539200 deprecation.py:339] From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "W1113 12:13:48.887746 140260436539200 deprecation.py:339] From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1113 12:13:50.593157 140260436539200 deprecation.py:339] From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "2021-11-13 12:13:52.718576: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-11-13 12:13:52.946762: W tensorflow/core/framework/dataset.cc:679] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "/home/snksynthesis/.local/lib/python3.8/site-packages/keras/backend.py:401: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:617: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "W1113 12:14:11.252840 140257217984256 deprecation.py:542] From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:617: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python tensorflow/models/research/object_detection/model_main_tf2.py --model_dir={MODEL_NAME} --pipeline_config_path={MODEL_NAME}/pipeline.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc60888-0358-449b-9643-5f155257b08a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predict from Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92ec9130-deb1-4fe2-9bb2-db6da7cd9799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'tensorflow/models/research/object_detection/model_main_tf2.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python tensorflow/models/research/object_detection/model_main_tf2.py --model_dir={MODEL_NAME} --pipeline_config_path={MODEL_NAME}/pipeline.config --checkpoint_dir={MODEL_NAME} "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98cd7bd2-e6f7-475d-af7e-f96b45681d0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Catch Basin Classifier (TFOD)\n",
    "This uses the Tensorflow Object Detection API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e52058-e5ed-48a0-b4f3-e2b01d08a9d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "166c4133-d603-48a4-bb85-17fb6aadaf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a43a05-c8e1-4acc-a9a2-b246613f5b33",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033da29b-5d1f-433e-8d2c-350d01f876a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8\"\n",
    "DOWNLOAD_URL = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f22bfd40-b505-4aba-a96c-9f9f276b3514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model...\n",
      "Model downloaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading model...\")\n",
    "r = requests.get(DOWNLOAD_URL)\n",
    "with open(MODEL_NAME + \".tar.gz\", \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "print(\"Model downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a007e79-13da-4cc1-bb2b-fbb5fa8d5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf {MODEL_NAME + \".tar.gz\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64478de0-cd8e-4386-8ee1-05288f7d2d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(str(Path(MODEL_NAME, \"saved_model\").absolute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24c3c043-604a-4c56-a4d0-c72ac3430ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject at 0x7f70e5c85c70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6382f16-592b-42ec-832f-f631c346ae8d",
   "metadata": {},
   "source": [
    "## Label Map\n",
    "`label_map.pbtxt` maps the class (name of number) to a number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99a17577-b97d-4cef-9fa8-d5872a2771a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: label_map.pbtxt: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat label_map.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e46f1c0-0a0d-4943-aae7-223b08cf70f6",
   "metadata": {},
   "source": [
    "Define some utility functions to convert from class to `int` and `int` to class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70b0866e-cabd-4cc4-935f-5012839604db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_to_int(class_name):\n",
    "    if class_name == \"blocked\":\n",
    "        return 1\n",
    "    elif class_name == \"partial\":\n",
    "        return 2\n",
    "    elif class_name == \"clear\":\n",
    "        return 3\n",
    "    else:\n",
    "        raise Exception(\"Invalid input\")\n",
    "\n",
    "\n",
    "def int_to_class(integer):\n",
    "    if integer == 1:\n",
    "        return \"blocked\"\n",
    "    elif integer == 2:\n",
    "        return \"partial\"\n",
    "    elif integer == 3:\n",
    "        return \"clear\"\n",
    "    else:\n",
    "        raise Exception(\"Invalid input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14247710-9d3e-4d12-929f-764b1e3b6cb1",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a386aef2-34f1-435e-bd68-3a44792add2d",
   "metadata": {},
   "source": [
    "Convert all PASCAL VOC (XML) files in `data/` to a CSV file, `labels.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0009df5-be8b-4358-ba49-cbc31681d18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to CSV...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import xml.etree.ElementTree as XMLElementTree\n",
    "\n",
    "print(\"Converting to CSV...\")\n",
    "columns = (\"filename\", \"class\", \"width\", \"height\", \"xmin\", \"ymin\", \"xmax\", \"ymax\")\n",
    "rows = []\n",
    "for filename in glob.glob('data/*.xml'):\n",
    "    parsed_obj = XMLElementTree.parse(filename)\n",
    "    root = parsed_obj.getroot()\n",
    "    filename = root.find(\"filename\").text\n",
    "    for obj in root.findall(\"object\"):\n",
    "        row = []\n",
    "        row.append(filename)\n",
    "        row.append(obj.find(\"name\").text)  # name => class\n",
    "        row.append(root.find(\"size\").find(\"width\").text)\n",
    "        row.append(root.find(\"size\").find(\"height\").text)\n",
    "        for i in obj.find(\"bndbox\"):\n",
    "            row.append(int(i.text))\n",
    "        rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "df.to_csv(\"labels.csv\", index=False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fdddaf-eeb1-4e71-b961-dd82ff2cea75",
   "metadata": {},
   "source": [
    "Load the CSV file with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae0a9336-05ef-475f-a4a8-7c0b19de447e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>class</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C13.JPG</td>\n",
       "      <td>clear</td>\n",
       "      <td>517</td>\n",
       "      <td>705</td>\n",
       "      <td>220</td>\n",
       "      <td>437</td>\n",
       "      <td>343</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B1.JPG</td>\n",
       "      <td>blocked</td>\n",
       "      <td>659</td>\n",
       "      <td>800</td>\n",
       "      <td>316</td>\n",
       "      <td>550</td>\n",
       "      <td>458</td>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C11.JPG</td>\n",
       "      <td>clear</td>\n",
       "      <td>526</td>\n",
       "      <td>702</td>\n",
       "      <td>216</td>\n",
       "      <td>509</td>\n",
       "      <td>359</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C15.JPG</td>\n",
       "      <td>clear</td>\n",
       "      <td>546</td>\n",
       "      <td>784</td>\n",
       "      <td>213</td>\n",
       "      <td>510</td>\n",
       "      <td>380</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B10.JPG</td>\n",
       "      <td>blocked</td>\n",
       "      <td>505</td>\n",
       "      <td>639</td>\n",
       "      <td>115</td>\n",
       "      <td>319</td>\n",
       "      <td>196</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename    class  width  height  xmin  ymin  xmax  ymax\n",
       "0  C13.JPG    clear    517     705   220   437   343   489\n",
       "1   B1.JPG  blocked    659     800   316   550   458   609\n",
       "2  C11.JPG    clear    526     702   216   509   359   591\n",
       "3  C15.JPG    clear    546     784   213   510   380   564\n",
       "4  B10.JPG  blocked    505     639   115   319   196   378"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"labels.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545ec7b-7e49-49fd-aa9e-ae093e46179a",
   "metadata": {},
   "source": [
    "Split data into test data and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2840cd52-62d5-49a3-8bdb-b75386192320",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.sample(frac=0.8, random_state=100)\n",
    "test_df = df.drop(train_df.index).sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dd3281-a7ab-4d20-9c98-96559ef26729",
   "metadata": {},
   "source": [
    "Create TF Records within `annotations/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f49d4e20-381e-4686-a0f4-4e4e83b17730",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"annotations\").exists():\n",
    "    !mkdir annotations\n",
    "\n",
    "def create_tf_record(dataframe, record_filename):\n",
    "    with tf.io.TFRecordWriter(str(Path(\"annotations\", record_filename))) as writer:\n",
    "        for index, row in dataframe.iterrows():\n",
    "            filename_encoded = row[\"filename\"].encode(\"utf-8\")\n",
    "            width = int(row[\"width\"])\n",
    "            height = int(row[\"height\"])\n",
    "            encoded_jpg = None\n",
    "            with tf.io.gfile.GFile(str(Path(\"data\") / row[\"filename\"]), \"rb\") as f:\n",
    "                encoded_jpg = f.read()\n",
    "            if encoded_jpg is None:\n",
    "                raise Exception(\"Unable to read image: \" + row[\"filename\"])\n",
    "\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                \"image/height\": dataset_util.int64_feature(height),\n",
    "                \"image/width\": dataset_util.int64_feature(width),\n",
    "                \"image/filename\": dataset_util.bytes_feature(filename_encoded),\n",
    "                \"image/source_id\": dataset_util.bytes_feature(filename_encoded),\n",
    "                \"image/encoded\": dataset_util.bytes_feature(encoded_jpg),\n",
    "                \"image/format\": dataset_util.bytes_feature(b\"jpg\"),\n",
    "                \"image/object/bbox/xmin\": dataset_util.float_list_feature([int(row[\"xmin\"]) / width]),\n",
    "                \"image/object/bbox/xmax\": dataset_util.float_list_feature([int(row[\"xmax\"]) / width]),\n",
    "                \"image/object/bbox/ymin\": dataset_util.float_list_feature([int(row[\"ymin\"]) / height]),\n",
    "                \"image/object/bbox/ymax\": dataset_util.float_list_feature([int(row[\"ymax\"]) / height]),\n",
    "                \"image/object/class/text\": dataset_util.bytes_list_feature([row[\"class\"].encode(\"utf-8\")]),\n",
    "                \"image/object/class/label\": dataset_util.int64_list_feature([class_to_int(row[\"class\"])]),\n",
    "            }))\n",
    "            writer.write(example.SerializeToString())\n",
    "\n",
    "# Train Data\n",
    "create_tf_record(train_df, \"train.record\")\n",
    "# Test Data\n",
    "create_tf_record(test_df, \"test.record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f2f84-d05c-48a8-a14e-c58aa12839c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Edit Model Configuration\n",
    "Edit `pipeline.config` to configure the model to be better for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b825627e-40bb-451a-bc0f-3d703e1134e8",
   "metadata": {},
   "source": [
    "Load `pipeline.config` into a python object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abc1e2be-1535-4cf5-ac96-8a9a1fe72681",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(str(Path(MODEL_NAME, \"pipeline.config\")), \"r\") as f:\n",
    "    text_format.Merge(f.read(), pipeline_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4a0211-c2cd-463c-87ee-7f38e1648369",
   "metadata": {},
   "source": [
    "Edit attributes of `pipeline_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff57645e-9eab-44f1-8097-d6a9eca87b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Classes: blocked, partial, and clear.\n",
    "pipeline_config.model.ssd.num_classes = 3\n",
    "# Set batch_size based on memory available.\n",
    "pipeline_config.train_config.batch_size = 4\n",
    "# Path to checkpoint of model\n",
    "pipeline_config.train_config.fine_tune_checkpoint = str(Path(MODEL_NAME, \"checkpoint0\", \"ckpt-0\"))\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline_config.train_input_reader.label_map_path = str(Path(\"annotations\", \"label_map.pbtxt\"))\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [str(Path(\"annotations\", \"train.record\"))]\n",
    "pipeline_config.eval_input_reader[0].label_map_path = str(Path(\"annotations\", \"label_map.pbtxt\"))\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [str(Path(\"annotations\", \"test.record\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc584754-4bbc-48ef-9f63-ae9b6df8573e",
   "metadata": {},
   "source": [
    "Save `pipeline_config` to `pipeline.config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33456dcf-d345-4156-8084-50ad6f02a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.gfile.GFile(str(Path(MODEL_NAME, \"pipeline.config\")), \"w\") as f:\n",
    "    config_text = text_format.MessageToString(pipeline_config)\n",
    "    f.write(config_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7967cf62-2336-4d4c-bab2-254d41567629",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34668a1b-f590-4acb-902c-695d622e74aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-18 19:43:59.063933: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/snksynthesis/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2021-11-18 19:43:59.063985: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "2021-11-18 19:44:01.570212: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: UNKNOWN ERROR (100)\n",
      "2021-11-18 19:44:01.570267: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Leni7_laptop): /proc/driver/nvidia/version does not exist\n",
      "2021-11-18 19:44:01.570733: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "W1118 19:44:01.572208 140104151586624 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "I1118 19:44:01.573602 140104151586624 mirrored_strategy.py:376] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "INFO:tensorflow:Maybe overwriting train_steps: None\n",
      "I1118 19:44:01.577191 140104151586624 config_util.py:552] Maybe overwriting train_steps: None\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I1118 19:44:01.577363 140104151586624 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/model_lib_v2.py:557: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "W1118 19:44:01.601037 140104151586624 deprecation.py:341] From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/model_lib_v2.py:557: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "INFO:tensorflow:Reading unweighted datasets: ['annotations/train.record']\n",
      "I1118 19:44:01.606168 140104151586624 dataset_builder.py:163] Reading unweighted datasets: ['annotations/train.record']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['annotations/train.record']\n",
      "I1118 19:44:01.606656 140104151586624 dataset_builder.py:80] Reading record datasets for input file: ['annotations/train.record']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I1118 19:44:01.606810 140104151586624 dataset_builder.py:81] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W1118 19:44:01.606953 140104151586624 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "W1118 19:44:01.609694 140104151586624 deprecation.py:341] From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W1118 19:44:01.636311 140104151586624 deprecation.py:341] From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1096: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W1118 19:44:07.386266 140104151586624 deprecation.py:341] From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1096: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1096: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "W1118 19:44:09.908617 140104151586624 deprecation.py:341] From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1096: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:465: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1118 19:44:11.298554 140104151586624 deprecation.py:341] From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:465: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "2021-11-18 19:44:13.421397: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2021-11-18 19:44:13.682404: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at multi_device_iterator_ops.cc:789 : NOT_FOUND: Resource AnonymousMultiDeviceIterator/AnonymousMultiDeviceIterator0/N10tensorflow4data12_GLOBAL__N_119MultiDeviceIteratorE does not exist.\n",
      "/home/snksynthesis/.local/lib/python3.8/site-packages/keras/backend.py:414: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "W1118 19:44:30.641619 140100616840960 deprecation.py:545] From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "INFO:tensorflow:Step 100 per-step time 4.594s\n",
      "I1118 19:52:09.625572 140104151586624 model_lib_v2.py:698] Step 100 per-step time 4.594s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.37041298,\n",
      " 'Loss/localization_loss': 0.30770808,\n",
      " 'Loss/regularization_loss': 0.15249512,\n",
      " 'Loss/total_loss': 0.8306162,\n",
      " 'learning_rate': 0.0319994}\n",
      "I1118 19:52:09.626065 140104151586624 model_lib_v2.py:701] {'Loss/classification_loss': 0.37041298,\n",
      " 'Loss/localization_loss': 0.30770808,\n",
      " 'Loss/regularization_loss': 0.15249512,\n",
      " 'Loss/total_loss': 0.8306162,\n",
      " 'learning_rate': 0.0319994}\n",
      "INFO:tensorflow:Step 200 per-step time 4.155s\n",
      "I1118 19:59:05.114017 140104151586624 model_lib_v2.py:698] Step 200 per-step time 4.155s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2831615,\n",
      " 'Loss/localization_loss': 0.14786059,\n",
      " 'Loss/regularization_loss': 0.15276875,\n",
      " 'Loss/total_loss': 0.5837908,\n",
      " 'learning_rate': 0.0373328}\n",
      "I1118 19:59:05.114400 140104151586624 model_lib_v2.py:701] {'Loss/classification_loss': 0.2831615,\n",
      " 'Loss/localization_loss': 0.14786059,\n",
      " 'Loss/regularization_loss': 0.15276875,\n",
      " 'Loss/total_loss': 0.5837908,\n",
      " 'learning_rate': 0.0373328}\n",
      "INFO:tensorflow:Step 300 per-step time 4.081s\n",
      "I1118 20:05:53.173252 140104151586624 model_lib_v2.py:698] Step 300 per-step time 4.081s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.29867938,\n",
      " 'Loss/localization_loss': 0.25579163,\n",
      " 'Loss/regularization_loss': 0.15278922,\n",
      " 'Loss/total_loss': 0.70726025,\n",
      " 'learning_rate': 0.0426662}\n",
      "I1118 20:05:53.173633 140104151586624 model_lib_v2.py:701] {'Loss/classification_loss': 0.29867938,\n",
      " 'Loss/localization_loss': 0.25579163,\n",
      " 'Loss/regularization_loss': 0.15278922,\n",
      " 'Loss/total_loss': 0.70726025,\n",
      " 'learning_rate': 0.0426662}\n",
      "INFO:tensorflow:Step 400 per-step time 4.059s\n",
      "I1118 20:12:39.096383 140104151586624 model_lib_v2.py:698] Step 400 per-step time 4.059s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.20278984,\n",
      " 'Loss/localization_loss': 0.13098663,\n",
      " 'Loss/regularization_loss': 0.15290776,\n",
      " 'Loss/total_loss': 0.48668423,\n",
      " 'learning_rate': 0.047999598}\n",
      "I1118 20:12:39.096723 140104151586624 model_lib_v2.py:701] {'Loss/classification_loss': 0.20278984,\n",
      " 'Loss/localization_loss': 0.13098663,\n",
      " 'Loss/regularization_loss': 0.15290776,\n",
      " 'Loss/total_loss': 0.48668423,\n",
      " 'learning_rate': 0.047999598}\n",
      "INFO:tensorflow:Step 500 per-step time 23.102s\n",
      "I1118 20:51:09.507344 140104151586624 model_lib_v2.py:698] Step 500 per-step time 23.102s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.14750667,\n",
      " 'Loss/localization_loss': 0.10711169,\n",
      " 'Loss/regularization_loss': 0.15330778,\n",
      " 'Loss/total_loss': 0.40792614,\n",
      " 'learning_rate': 0.053333}\n",
      "I1118 20:51:09.583656 140104151586624 model_lib_v2.py:701] {'Loss/classification_loss': 0.14750667,\n",
      " 'Loss/localization_loss': 0.10711169,\n",
      " 'Loss/regularization_loss': 0.15330778,\n",
      " 'Loss/total_loss': 0.40792614,\n",
      " 'learning_rate': 0.053333}\n",
      "INFO:tensorflow:Step 600 per-step time 7.959s\n",
      "I1118 21:04:25.524170 140104151586624 model_lib_v2.py:698] Step 600 per-step time 7.959s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.21450792,\n",
      " 'Loss/localization_loss': 0.14871305,\n",
      " 'Loss/regularization_loss': 0.15370971,\n",
      " 'Loss/total_loss': 0.5169307,\n",
      " 'learning_rate': 0.0586664}\n",
      "I1118 21:04:25.547328 140104151586624 model_lib_v2.py:701] {'Loss/classification_loss': 0.21450792,\n",
      " 'Loss/localization_loss': 0.14871305,\n",
      " 'Loss/regularization_loss': 0.15370971,\n",
      " 'Loss/total_loss': 0.5169307,\n",
      " 'learning_rate': 0.0586664}\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python tensorflow/models/research/object_detection/model_main_tf2.py --model_dir={MODEL_NAME} --pipeline_config_path={MODEL_NAME}/pipeline.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc60888-0358-449b-9643-5f155257b08a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predict from Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ec9130-deb1-4fe2-9bb2-db6da7cd9799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-18 22:05:22.020201: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-18 22:05:22.020259: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "2021-11-18 22:05:24.621712: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: UNKNOWN ERROR (100)\n",
      "2021-11-18 22:05:24.621774: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Leni7_laptop): /proc/driver/nvidia/version does not exist\n",
      "WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n",
      "W1118 22:05:24.625172 139708893472576 model_lib_v2.py:1081] Forced number of epochs for all eval validations to be 1.\n",
      "INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: None\n",
      "I1118 22:05:24.625405 139708893472576 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: None\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I1118 22:05:24.625524 139708893472576 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n",
      "I1118 22:05:24.625631 139708893472576 config_util.py:552] Maybe overwriting eval_num_epochs: 1\n",
      "WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
      "W1118 22:05:24.625766 139708893472576 model_lib_v2.py:1099] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
      "2021-11-18 22:05:24.629448: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:tensorflow:Reading unweighted datasets: ['annotations/test.record']\n",
      "I1118 22:05:24.662910 139708893472576 dataset_builder.py:163] Reading unweighted datasets: ['annotations/test.record']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['annotations/test.record']\n",
      "I1118 22:05:24.663307 139708893472576 dataset_builder.py:80] Reading record datasets for input file: ['annotations/test.record']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I1118 22:05:24.663455 139708893472576 dataset_builder.py:81] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W1118 22:05:24.663576 139708893472576 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "W1118 22:05:24.665361 139708893472576 deprecation.py:341] From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W1118 22:05:24.689286 139708893472576 deprecation.py:341] From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1096: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W1118 22:05:28.097934 139708893472576 deprecation.py:341] From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1096: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:465: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1118 22:05:29.067655 139708893472576 deprecation.py:341] From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:465: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Waiting for new checkpoint at ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8\n",
      "I1118 22:05:31.488367 139708893472576 checkpoint_utils.py:140] Waiting for new checkpoint at ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8\n",
      "INFO:tensorflow:Found new checkpoint at ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/ckpt-1\n",
      "I1118 22:05:31.489221 139708893472576 checkpoint_utils.py:149] Found new checkpoint at ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/ckpt-1\n",
      "/home/snksynthesis/.local/lib/python3.8/site-packages/keras/backend.py:414: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/eval_util.py:929: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1118 22:05:51.400658 139708893472576 deprecation.py:341] From /home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/eval_util.py:929: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Finished eval step 0\n",
      "I1118 22:05:51.409279 139708893472576 model_lib_v2.py:958] Finished eval step 0\n",
      "WARNING:tensorflow:From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:465: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "W1118 22:05:51.522013 139708893472576 deprecation.py:341] From /home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:465: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "INFO:tensorflow:Performing evaluation on 8 images.\n",
      "I1118 22:05:54.030318 139708893472576 coco_evaluation.py:293] Performing evaluation on 8 images.\n",
      "creating index...\n",
      "index created!\n",
      "INFO:tensorflow:Loading and preparing annotation results...\n",
      "I1118 22:05:54.030661 139708893472576 coco_tools.py:116] Loading and preparing annotation results...\n",
      "INFO:tensorflow:DONE (t=0.00s)\n",
      "I1118 22:05:54.032303 139708893472576 coco_tools.py:138] DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Traceback (most recent call last):\n",
      "  File \"tensorflow/models/research/object_detection/model_main_tf2.py\", line 114, in <module>\n",
      "    tf.compat.v1.app.run()\n",
      "  File \"/home/snksynthesis/.local/lib/python3.8/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/home/snksynthesis/.local/lib/python3.8/site-packages/absl/app.py\", line 303, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/home/snksynthesis/.local/lib/python3.8/site-packages/absl/app.py\", line 251, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"tensorflow/models/research/object_detection/model_main_tf2.py\", line 81, in main\n",
      "    model_lib_v2.eval_continuously(\n",
      "  File \"/home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/model_lib_v2.py\", line 1151, in eval_continuously\n",
      "    eager_eval_loop(\n",
      "  File \"/home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/model_lib_v2.py\", line 1002, in eager_eval_loop\n",
      "    eval_metrics.update(evaluator.evaluate())\n",
      "  File \"/home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/metrics/coco_evaluation.py\", line 302, in evaluate\n",
      "    box_evaluator = coco_tools.COCOEvalWrapper(\n",
      "  File \"/home/snksynthesis/.local/lib/python3.8/site-packages/object_detection/metrics/coco_tools.py\", line 207, in __init__\n",
      "    cocoeval.COCOeval.__init__(self, groundtruth, detections, iouType=iou_type)\n",
      "  File \"/home/snksynthesis/.local/lib/python3.8/site-packages/pycocotools/cocoeval.py\", line 76, in __init__\n",
      "    self.params = Params(iouType=iouType) # parameters\n",
      "  File \"/home/snksynthesis/.local/lib/python3.8/site-packages/pycocotools/cocoeval.py\", line 527, in __init__\n",
      "    self.setDetParams()\n",
      "  File \"/home/snksynthesis/.local/lib/python3.8/site-packages/pycocotools/cocoeval.py\", line 507, in setDetParams\n",
      "    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n",
      "  File \"<__array_function__ internals>\", line 5, in linspace\n",
      "  File \"/home/snksynthesis/.local/lib/python3.8/site-packages/numpy/core/function_base.py\", line 113, in linspace\n",
      "    num = operator.index(num)\n",
      "TypeError: 'numpy.float64' object cannot be interpreted as an integer\n"
     ]
    }
   ],
   "source": [
    "!python tensorflow/models/research/object_detection/model_main_tf2.py --model_dir={MODEL_NAME} --pipeline_config_path={MODEL_NAME}/pipeline.config --checkpoint_dir={MODEL_NAME} "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
